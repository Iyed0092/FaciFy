{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99fb8cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iyedm\\OneDrive\\Desktop\\facify\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "os.chdir(r\"C:\\Users\\iyedm\\OneDrive\\Desktop\\facify\")\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7ea585d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0+cu128\n",
      "CUDA available: True\n",
      "Number of CUDA devices: 1\n",
      "Current CUDA device: NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check PyTorch version\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "\n",
    "# Check if CUDA is available\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "# Show CUDA device count\n",
    "print(\"Number of CUDA devices:\", torch.cuda.device_count())\n",
    "\n",
    "# Show the name of the current CUDA device\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Current CUDA device:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0db43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from backend.models.stylegan3_conditional import ConditionalStyleGAN2Generator\n",
    "from backend.models.stylegan3_conditional_discriminator import ConditionalDiscriminator\n",
    "\n",
    "# ----------------------------\n",
    "# Paths\n",
    "# ----------------------------\n",
    "RAW_IMAGES_DIR = \"data/processed/faces\"           # preprocessed 128x128 images\n",
    "EMBEDDINGS_PATH = \"data/embeddings/faces.npy\"    # embeddings\n",
    "PRETRAINED_PKL = \"backend/models/model_weights/stylegan2-ffhq-512x512.pkl\"\n",
    "SAVE_WEIGHTS = \"backend/models/model_weights/stylegan2_conditional.pth\"\n",
    "\n",
    "# ----------------------------\n",
    "# Training hyperparameters\n",
    "# ----------------------------\n",
    "BATCH_SIZE = 4\n",
    "LR = 1e-4\n",
    "EPOCHS = 5\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"[INFO] Using device: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "477cb0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "\n",
    "class FacesDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset that yields (image_tensor, embedding_tensor) pairs.\n",
    "    Expects embeddings saved as: np.save(path, {\"embeddings\": embeddings_array, \"filenames\": filenames})\n",
    "    - embeddings_array shape: (N, 512)\n",
    "    - filenames: list of filenames corresponding to rows of embeddings_array\n",
    "    \"\"\"\n",
    "    def __init__(self, images_dir, embeddings_path, transform=None):\n",
    "        self.images_dir = images_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # Load the .npy which contains a dict\n",
    "        print(f\"[DEBUG] Loading embeddings from: {embeddings_path}\")\n",
    "        loaded = np.load(embeddings_path, allow_pickle=True)\n",
    "        # np.load may return an array-like with a dict inside; handle both possibilities\n",
    "        if isinstance(loaded, np.ndarray) and loaded.dtype == object:\n",
    "            data = loaded.item()\n",
    "        elif isinstance(loaded, dict):\n",
    "            data = loaded\n",
    "        else:\n",
    "            # If someone saved plain array (rare), assume it's embeddings without filenames\n",
    "            raise ValueError(\"[ERROR] Unexpected embeddings file format. Expected dict with 'embeddings' and 'filenames'.\")\n",
    "\n",
    "        if \"embeddings\" not in data or \"filenames\" not in data:\n",
    "            raise ValueError(\"[ERROR] Embeddings file must contain keys 'embeddings' and 'filenames'.\")\n",
    "\n",
    "        embeddings_array = np.asarray(data[\"embeddings\"])\n",
    "        filenames_list = list(data[\"filenames\"])\n",
    "\n",
    "        print(f\"[DEBUG] Embeddings loaded: {embeddings_array.shape[0]} items, embeddings dim = {embeddings_array.shape[1] if embeddings_array.ndim>1 else 'unknown'}\")\n",
    "\n",
    "        # Build mapping filename -> embedding (filenames in embeddings may include paths or just names)\n",
    "        self.embedding_map = {}\n",
    "        for fname, emb in zip(filenames_list, embeddings_array):\n",
    "            # ensure the filename is exactly the basename\n",
    "            base = os.path.basename(fname)\n",
    "            self.embedding_map[base] = emb\n",
    "\n",
    "        # Now scan images_dir and keep only files that have embeddings\n",
    "        all_image_files = sorted([f for f in os.listdir(images_dir) if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))])\n",
    "        kept = []\n",
    "        skipped_no_embedding = []\n",
    "        for f in all_image_files:\n",
    "            if f in self.embedding_map:\n",
    "                kept.append(f)\n",
    "            else:\n",
    "                skipped_no_embedding.append(f)\n",
    "\n",
    "        if len(skipped_no_embedding) > 0:\n",
    "            print(f\"[WARN] {len(skipped_no_embedding)} images in '{images_dir}' have NO matching embedding and will be skipped (examples): {skipped_no_embedding[:5]}\")\n",
    "\n",
    "        if len(kept) == 0:\n",
    "            raise RuntimeError(f\"[ERROR] No images in '{images_dir}' matched embeddings from '{embeddings_path}'.\")\n",
    "\n",
    "        self.filenames = kept\n",
    "        # create a parallel list of embeddings in the same order as self.filenames\n",
    "        self.embeddings = [self.embedding_map[f] for f in self.filenames]\n",
    "\n",
    "        print(f\"[INFO] Using {len(self.filenames)} image-embedding pairs for training.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.images_dir, self.filenames[idx])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        # embeddings stored as numpy arrays; convert to torch tensor\n",
    "        embedding = torch.tensor(self.embeddings[idx], dtype=torch.float)\n",
    "        return image, embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ba92b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Loading embeddings from: data/embeddings/faces.npy\n",
      "[DEBUG] Embeddings loaded: 7136 items, embeddings dim = 512\n",
      "[WARN] 83 images in 'data/processed/faces' have NO matching embedding and will be skipped (examples): ['1 (1047).jpg', '1 (120).jpg', '1 (1323).jpg', '1 (1519).jpg', '1 (1568).jpg']\n",
      "[INFO] Using 7136 image-embedding pairs for training.\n",
      "[INFO] Dataloader ready. 7136 samples, batch_size=4\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "dataset = FacesDataset(\"data/processed/faces\", \"data/embeddings/faces.npy\", transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(f\"[INFO] Dataloader ready. {len(dataset)} samples, batch_size={dataloader.batch_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80dcedf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Creating dataset + dataloader...\n",
      "[DEBUG] Loading embeddings from: data/embeddings/faces.npy\n",
      "[DEBUG] Embeddings loaded: 7136 items, embeddings dim = 512\n",
      "[WARN] 83 images in 'data/processed/faces' have NO matching embedding and will be skipped (examples): ['1 (1047).jpg', '1 (120).jpg', '1 (1323).jpg', '1 (1519).jpg', '1 (1568).jpg']\n",
      "[INFO] Using 7136 image-embedding pairs for training.\n",
      "[INFO] Saved sample image grid to: debug_samples\\sample_grid.png\n",
      "\n",
      "[INFO] First 3 samples details:\n",
      "  1) filename: 1 (1).jpeg\n",
      "     image tensor shape: (3, 128, 128)\n",
      "     embedding shape: (512,)\n",
      "     embedding L2 norm: 1.0000\n",
      "  2) filename: 1 (1).jpg\n",
      "     image tensor shape: (3, 128, 128)\n",
      "     embedding shape: (512,)\n",
      "     embedding L2 norm: 1.0000\n",
      "  3) filename: 1 (1).png\n",
      "     image tensor shape: (3, 128, 128)\n",
      "     embedding shape: (512,)\n",
      "     embedding L2 norm: 1.0000\n",
      "\n",
      "[INFO] Dataset length: 7136 (should be 7136)\n"
     ]
    }
   ],
   "source": [
    "# backend/scripts/debug_dataset.py\n",
    "import os\n",
    "import torch\n",
    "from torchvision.utils import make_grid, save_image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Import the fixed dataset class (adjust path if you placed it elsewhere)\n",
    "\n",
    "DEBUG_OUT_DIR = \"debug_samples\"\n",
    "os.makedirs(DEBUG_OUT_DIR, exist_ok=True)\n",
    "\n",
    "# same transform used for training (images normalized to [-1,1])\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "print(\"[DEBUG] Creating dataset + dataloader...\")\n",
    "dataset = FacesDataset(\"data/processed/faces\", \"data/embeddings/faces.npy\", transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "# get a small batch\n",
    "images, embeddings = next(iter(dataloader))\n",
    "\n",
    "# denormalize images for saving/viewing\n",
    "images_denorm = images * 0.5 + 0.5  # from [-1,1] -> [0,1]\n",
    "grid = make_grid(images_denorm[:3], nrow=3)  # first 3 images\n",
    "\n",
    "save_path = os.path.join(DEBUG_OUT_DIR, \"sample_grid.png\")\n",
    "save_image(grid, save_path)\n",
    "print(f\"[INFO] Saved sample image grid to: {save_path}\")\n",
    "\n",
    "# print details for first 3 samples\n",
    "print(\"\\n[INFO] First 3 samples details:\")\n",
    "for i in range(min(3, images.size(0))):\n",
    "    # filename\n",
    "    fname = dataset.filenames[i]\n",
    "    print(f\"  {i+1}) filename: {fname}\")\n",
    "    print(f\"     image tensor shape: {tuple(images[i].shape)}\")\n",
    "    print(f\"     embedding shape: {tuple(embeddings[i].shape)}\")\n",
    "    norm = torch.norm(embeddings[i]).item()\n",
    "    print(f\"     embedding L2 norm: {norm:.4f}\")\n",
    "\n",
    "# Quick check: ensure number of images == number of embeddings used\n",
    "print(f\"\\n[INFO] Dataset length: {len(dataset)} (should be 7136)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ba9743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cuda\n",
      "[DEBUG] Loading embeddings from: data/embeddings/faces.npy\n",
      "[DEBUG] Embeddings loaded: 7136 items, embeddings dim = 512\n",
      "[WARN] 83 images in 'data/processed/faces' have NO matching embedding and will be skipped (examples): ['1 (1047).jpg', '1 (120).jpg', '1 (1323).jpg', '1 (1519).jpg', '1 (1568).jpg']\n",
      "[INFO] Using 7136 image-embedding pairs for training.\n",
      "[INFO] Dataloader ready: 7136 samples, batch_size=4\n",
      "[INFO] Generator and Discriminator instantiated on GPU.\n",
      "[INFO] Loading pretrained .pkl from: backend/models/model_weights/stylegan2-ffhq-512x512.pkl\n",
      "[ERROR] Exception loading .pkl: No module named 'torch_utils'. Continuing without weights.\n",
      "[INFO] Starting dry-run training on GPU...\n"
     ]
    }
   ],
   "source": [
    "# backend/scripts/train_dry_run_gpu.py\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "\n",
    "# Import your dataset and models (adjust paths if needed)\n",
    "from backend.models.stylegan3_conditional import ConditionalStyleGAN2Generator\n",
    "from backend.models.stylegan3_conditional_discriminator import ConditionalDiscriminator\n",
    "\n",
    "# -------------------\n",
    "# GPU setup\n",
    "# -------------------\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"[INFO] Using device: {DEVICE}\")\n",
    "torch.backends.cudnn.benchmark = True  # optimize conv layers for GPU\n",
    "\n",
    "# -------------------\n",
    "# Config\n",
    "# -------------------\n",
    "IMAGES_DIR = \"data/processed/faces\"\n",
    "EMBEDDINGS_PATH = \"data/embeddings/faces.npy\"\n",
    "PRETRAINED_PKL = \"backend/models/model_weights/stylegan2-ffhq-512x512.pkl\"\n",
    "BATCH_SIZE = 4\n",
    "NUM_BATCHES = 2  # dry-run\n",
    "LR = 1e-4\n",
    "\n",
    "# -------------------\n",
    "# Dataset + DataLoader\n",
    "# -------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "dataset = FacesDataset(IMAGES_DIR, EMBEDDINGS_PATH, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                        num_workers=4, pin_memory=True)\n",
    "print(f\"[INFO] Dataloader ready: {len(dataset)} samples, batch_size={BATCH_SIZE}\")\n",
    "\n",
    "# -------------------\n",
    "# Models\n",
    "# -------------------\n",
    "gen = ConditionalStyleGAN2Generator(latent_dim=512, embed_dim=512, img_channels=3).to(DEVICE)\n",
    "disc = ConditionalDiscriminator(img_channels=3, embed_dim=512).to(DEVICE)\n",
    "print(\"[INFO] Generator and Discriminator instantiated on GPU.\")\n",
    "\n",
    "# -------------------\n",
    "# Load pretrained weights (best-effort)\n",
    "# -------------------\n",
    "if os.path.exists(PRETRAINED_PKL):\n",
    "    try:\n",
    "        print(f\"[INFO] Loading pretrained .pkl from: {PRETRAINED_PKL}\")\n",
    "        with open(PRETRAINED_PKL, \"rb\") as f:\n",
    "            ckpt = pickle.load(f)\n",
    "        candidate_keys = list(ckpt.keys())\n",
    "        print(f\"[DEBUG] Keys in .pkl: {candidate_keys[:10]} ...\")\n",
    "        g_state = None\n",
    "        for k in (\"G_ema\", \"G\", \"g_ema\", \"g\"):\n",
    "            if k in ckpt:\n",
    "                print(f\"[INFO] Found generator key: {k}\")\n",
    "                if isinstance(ckpt[k], dict) and all(isinstance(v, (torch.Tensor, type(None))) for v in ckpt[k].values()):\n",
    "                    g_state = ckpt[k]\n",
    "                elif hasattr(ckpt[k], \"state_dict\"):\n",
    "                    g_state = ckpt[k].state_dict()\n",
    "                break\n",
    "        if g_state:\n",
    "            res = gen.load_state_dict(g_state, strict=False)\n",
    "            try:\n",
    "                missing = res.missing_keys\n",
    "                unexpected = res.unexpected_keys\n",
    "            except Exception:\n",
    "                missing = res.get(\"missing_keys\", [])\n",
    "                unexpected = res.get(\"unexpected_keys\", [])\n",
    "            print(f\"[INFO] load_state_dict done. missing_keys={len(missing)}, unexpected_keys={len(unexpected)}\")\n",
    "            if missing: print(f\"[DEBUG] missing keys: {missing[:10]}\")\n",
    "            if unexpected: print(f\"[DEBUG] unexpected keys: {unexpected[:10]}\")\n",
    "        else:\n",
    "            print(\"[WARN] No usable generator state_dict found; proceeding without pretrained weights.\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Exception loading .pkl: {e}. Continuing without weights.\")\n",
    "else:\n",
    "    print(\"[WARN] Pretrained .pkl not found; training from scratch.\")\n",
    "\n",
    "# -------------------\n",
    "# Optimizers & Loss\n",
    "# -------------------\n",
    "opt_G = torch.optim.Adam(gen.parameters(), lr=LR, betas=(0.0, 0.99))\n",
    "opt_D = torch.optim.Adam(disc.parameters(), lr=LR, betas=(0.0, 0.99))\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# -------------------\n",
    "# Dry-run training loop\n",
    "# -------------------\n",
    "print(\"[INFO] Starting dry-run training on GPU...\")\n",
    "for batch_idx, (images, embeddings) in enumerate(dataloader, 1):\n",
    "    images = images.to(DEVICE, non_blocking=True)\n",
    "    embeddings = embeddings.to(DEVICE, non_blocking=True)\n",
    "\n",
    "    # ----- Discriminator step -----\n",
    "    opt_D.zero_grad()\n",
    "    with torch.no_grad():\n",
    "        fake = gen(embeddings)\n",
    "    # Upsample if necessary\n",
    "    if fake.shape[2:] != images.shape[2:]:\n",
    "        fake = F.interpolate(fake, size=images.shape[2:], mode=\"bilinear\", align_corners=False)\n",
    "        print(f\"[DEBUG] Upsampled fake to {tuple(fake.shape)}\")\n",
    "\n",
    "    real_labels = torch.ones(images.size(0), 1, device=DEVICE)\n",
    "    fake_labels = torch.zeros(images.size(0), 1, device=DEVICE)\n",
    "\n",
    "    real_out = disc(images, embeddings)\n",
    "    fake_out = disc(fake.detach(), embeddings)\n",
    "    loss_D = 0.5 * (criterion(real_out, real_labels) + criterion(fake_out, fake_labels))\n",
    "    loss_D.backward()\n",
    "    opt_D.step()\n",
    "\n",
    "    # ----- Generator step -----\n",
    "    opt_G.zero_grad()\n",
    "    fake = gen(embeddings)\n",
    "    if fake.shape[2:] != images.shape[2:]:\n",
    "        fake = F.interpolate(fake, size=images.shape[2:], mode=\"bilinear\", align_corners=False)\n",
    "    fake_out = disc(fake, embeddings)\n",
    "    loss_G = criterion(fake_out, real_labels)\n",
    "    loss_G.backward()\n",
    "    opt_G.step()\n",
    "\n",
    "    print(f\"[DEBUG] Batch {batch_idx}: images {tuple(images.shape)}, embeddings {tuple(embeddings.shape)}\")\n",
    "    print(f\"[DEBUG] losses -> loss_D={loss_D.item():.6f}, loss_G={loss_G.item():.6f}\")\n",
    "\n",
    "    if batch_idx >= 2:  # stop after dry-run\n",
    "        break\n",
    "\n",
    "print(\"[INFO] Dry-run complete. Models ran on GPU successfully, no weights saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a76e72a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
